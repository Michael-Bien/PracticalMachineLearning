---
title: "Practical Machine Learning - Fitness Activity Prediction with GBM"
author: "Michael Bien"
date: "5/26/2021"
output: 
  html_document:
    keep_md: yes
---

# Synopsis

In the analysis below, I fit a model to predict the type of activity being done, using numerous metrics recorded from a fitness band.  The dataset is filled with records of an individual performing an activity in one of 5 ways as defined in the variable CLASSE:

  1. according to specs
  2. throwing elbows forward
  3. lifting dumbbell halfway
  4. lowering dumbbell halfway
  5. throwing hips forward

 

To predict the activity type, I followed the following steps in the model building process:

**Steps of the Model Build**

  1. Data Import and Cleaning
  2. Pre-processing and Standardization
  3. Exploratory Data Analysis
  4. Model Fitting with Cross Validation
  5. Model Validation and OOS estimation
  6. Prediction on Holdout dataset

## Setup

Start by setting (echo = TRUE) and declaring libraries to be used

```{r setup, include=TRUE, message=FALSE, cache = TRUE}

knitr::opts_chunk$set(echo = TRUE, warning = FALSE)  #"always use echo = TRUE" for this assignment, per notes

library(dplyr)
library(caret)
library(ggplot2)

```
## Data Import and Cleaning

In the data processing step, I import the data, remove columns that are nearly all NA, and remove columns that are near zero variance.  There's a segment unused in the final model fit that downsamples the data since the data size resulted in slower exploratory run times.

```{r data_step, cache = TRUE}

  training <- read.csv("pml-training.csv")
  testing <- read.csv("pml-testing.csv")
  training <- training[,-(1:7)]  #remove columns that aren't independent predictor variables
  training <- training %>%       #reorder to move classe to front of data frame
    select(classe, everything())
  training$classe <- factor(training$classe)
  training <- training[,colMeans(is.na(training))<0.97] #select those that aren't all na's
  near_zero_variance_chars_to_drop <- nearZeroVar(training)
  training_trimmed <- training[,-near_zero_variance_chars_to_drop]
  
  ## this step was not used in the final fit, but helped make the process faster
  ## a downsampled dataset can be used to perform exploratory model fits
  downSampleIndicator <- createDataPartition(y=training_trimmed$classe, p=1.0, list=FALSE)
  downSampledData <- training_trimmed[downSampleIndicator,]

```
## Pre-processing and standardizing

Here, I separate the training data into training and validation sets, using a 80/20 split.  I standardize the independent variables the model uses to the same scale and centering. 

```{r preprocessing_step, cache = TRUE}

###SET SEED###
set.seed(16)
###Training/Validation###
inTrain <- createDataPartition(y=downSampledData$classe, p=0.80, list=FALSE)
training <- downSampledData[inTrain,]
validation <- downSampledData[-inTrain,]
###Preprocessing to center and scale###

training_copy <- training[,-1]
validation_copy <- validation[,-1]
preObj <- preProcess(training_copy, method=c("center", "scale"))
training_standardized <- predict(preObj, training_copy)
round(sapply(training_standardized[,1:10], mean),1)
validation_standardized <- predict(preObj, validation_copy)
training_standardized <- cbind(training$classe,training_standardized)
validation_standardized <- cbind(validation$classe,validation_standardized)
training_standardized <- training_standardized %>%
  rename(classe = `training$classe`)
validation_standardized <- validation_standardized %>%
  rename(classe = `validation$classe`)
```

## Exploratory data analysis

Here are a few charts showing how measurements vary by the "CLASSE" variable.  It is notable how some "CLASSE" patterns are different in the resulting measurements.  For example, we can note that the forearm pitch is different in CLASSE C.

```{r figs, cache = TRUE}

ggplot(data=training_standardized, aes(x=pitch_forearm, group=classe, fill=classe)) +
  geom_density()+
  facet_wrap(~classe)+
  ggtitle("PITCH_FOREARM measurement by CLASSE")

qplot(roll_belt, color=classe, data=training_standardized, geom="density", main="ROLL_BELT measurement by CLASSE")
qplot(yaw_belt, color=classe, data=training_standardized, geom="density", main="YAW BELT measurement by CLASSE")

```
## Model Fitting with Cross Validation

In this segment, I use 10 fold cross validation to fit a GBM model and then plot important variables and performance measures.  The accuracy on cross validation with a depth of 3 trees is 95% + .

```{r model_fitting_step, cache = TRUE}
#method: The resampling method: "boot", "boot632", "optimism_boot", "boot_all", "cv", "repeatedcv",
#number: Either the number of folds or number of resampling iterations
model_control <- trainControl(method="cv", number=10)
#GBM#
modFit_gbm <- train(classe ~ ., method="gbm", data=training_standardized, trControl = model_control, verbose=FALSE)
#Variable Importance chart#
summary(modFit_gbm)
#Cross validation accuracy by # of Boosting Iterations
plot(modFit_gbm)
```

## OOS Estimation

Here I predict the model accuracy on the validation dataset, which represents out of sample error.  The OOS accuracy is 96%.

```{r oos_estimation_step, cache = TRUE}
model_CFM <- confusionMatrix(predict(modFit_gbm, validation_standardized), factor(validation_standardized$classe))
model_CFM
```
## Testing Dataset Prediction

Finally, I predict the results on the given TESTING dataset.

```{r testing_prediction_step, cache = TRUE}
## Predict on the testing dataset ##
testing_copy <- testing
testing_standardized <- predict(preObj, testing_copy)
predict(modFit_gbm, testing_standardized)

```